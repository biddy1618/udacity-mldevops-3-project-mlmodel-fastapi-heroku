{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Deploying a Machine Learning Model on Heroku with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "This data was extracted from the census bureau database found at http://www.census.gov/ftp/pub/DES/www/welcome.html\n",
    "```\n",
    "Donor: Ronny Kohavi and Barry Becker,\n",
    "       Data Mining and Visualization\n",
    "       Silicon Graphics.\n",
    "       e-mail: ronnyk@sgi.com for questions.\n",
    "```\n",
    "* Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).\n",
    "* 48842 instances, mix of continuous and discrete    (train=32561, test=16281)\n",
    "* 45222 if instances with unknown values are removed (train=30162, test=15060)\n",
    "* Duplicate or conflicting instances : 6\n",
    "* Class probabilities for adult.all file\n",
    "  * Probability for the label '>50K'  : 23.93% / 24.78% (without unknowns)\n",
    "  * Probability for the label '<=50K' : 76.07% / 75.22% (without unknowns)\n",
    "\n",
    "For this project, only __train set__ of __unfiltered instanes__ is used - __32561 instances__.\n",
    "\n",
    "### Objective\n",
    "Prediction task is to determine whether a person makes over 50K a year.\n",
    "\n",
    "\n",
    "### Transformations\n",
    "Conversion of original data as follows:\n",
    "1. Discretized gross income into two ranges with threshold 50,000.\n",
    "2. Convert U.S. to US to avoid periods.\n",
    "3. Convert Unknown to \"?\"\n",
    "4. Run MLC++ GenCVFiles to generate data,test.\n",
    "\n",
    "\n",
    "### Description of fnlwgt (final weight)\n",
    "The weights on the CPS files are controlled to independent estimates of the\n",
    "civilian noninstitutional population of the US.  These are prepared monthly\n",
    "for us by Population Division here at the Census Bureau.  We use 3 sets of\n",
    "controls.\n",
    "These are:\n",
    "1. A single cell estimate of the population 16+ for each state.\n",
    "2. Controls for Hispanic Origin by age and sex.\n",
    "3. Controls by Race, age and sex.\n",
    "We use all three sets of controls in our weighting program and \"rake\" through\n",
    "them 6 times so that by the end we come back to all the controls we used.\n",
    "The term estimate refers to population totals derived from CPS by creating\n",
    "\"weighted tallies\" of any specified socio-economic characteristics of the\n",
    "population.\n",
    "\n",
    "People with similar demographic characteristics should have\n",
    "similar weights.  There is one important caveat to remember\n",
    "about this statement.  That is that since the CPS sample is\n",
    "actually a collection of 51 state samples, each with its own\n",
    "probability of selection, the statement only applies within\n",
    "state.\n",
    "\n",
    "### Data values\n",
    "| name | values |\n",
    "|------|--------|\n",
    "| __salary__ | >50K, <=50K.|\n",
    "| age | continuous |\n",
    "| workclass | Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked |\n",
    "| fnlwgt | continuous |\n",
    "| education | Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool |\n",
    "| education-num | continuous |\n",
    "| marital-status | Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse |\n",
    "| occupation | Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces |\n",
    "| relationship | Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried |\n",
    "| race | White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black |\n",
    "| sex | Female, Male |\n",
    "| capital-gain | continuous |\n",
    "| capital-loss | continuous |\n",
    "| hours-per-week | continuous |\n",
    "| native-country | United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/census.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trailing spaces: \\n{df.columns.tolist()[:5]} ...')\n",
    "def trim(dataset: pd.DataFrame, asdf: float = 0.0):\n",
    "    dataset.columns = [col.strip() for col in dataset.columns]\n",
    "    dataset.columns = [col.replace('-', '_') for col in dataset.columns]\n",
    "\n",
    "    return dataset.applymap(\n",
    "        lambda value: value.strip() if isinstance(value, str) else value)\n",
    "\n",
    "df = trim(df)\n",
    "print(f'Removing spaces: \\n{df.columns.tolist()[:5]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace `?` with `Unknown` for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_question_row(row):\n",
    "    return row.astype(str).str.contains('?', regex=False).any()\n",
    "\n",
    "df.loc[df.apply(mark_question_row, axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows containing `?`: {df.apply(mark_question_row, axis=1).sum()}')\n",
    "df.replace({'?': 'Unknown'}, inplace = True)\n",
    "print(f'Number of rows containing `?` after replacement: {df.apply(mark_question_row, axis=1).sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(keep = False)].sort_values('fnlgt').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {df.shape[0]}')\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(f'Number of rows after duplicates drop: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas_profiling` for initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL (for baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 48\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns' types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`education` and `education-num` have Pearson's correlation of 1.0, but `education_num` has ordinal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "num_features = ['age', 'fnlgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "target = 'salary'\n",
    "\n",
    "df = df[cat_features + num_features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal columns' order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass_order = ['Private', 'Self-emp-inc', 'Self-emp-not-inc', 'Local-gov', 'State-gov', 'Federal-gov', 'Without-pay', 'Never-worked', 'Unknown', 'Other']\n",
    "education_order = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', 'HS-grad', 'Some-college', 'Assoc-voc', 'Assoc-acdm', 'Bachelors', 'Masters', 'Prof-school', 'Doctorate', 'Other']\n",
    "marital_status_order = ['Married-civ-spouse', 'Married-AF-spouse', 'Married-spouse-absent', 'Separated', 'Divorced', 'Widowed', 'Never-married', 'Other']\n",
    "relationship_order = ['Husband', 'Wife', 'Own-child', 'Unmarried', 'Not-in-family', 'Other-relative', 'Other']\n",
    "\n",
    "cat_order = {\n",
    "    'workclass': workclass_order,\n",
    "    'education': education_order,\n",
    "    'marital_status': marital_status_order,\n",
    "    'occupation': None,\n",
    "    'relationship': relationship_order,\n",
    "    'race': None,\n",
    "    'sex': None,\n",
    "    'native_country': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y = df[target]\n",
    "X = df.loc[:, df.columns != target]\n",
    "\n",
    "target_preprocessor = LabelBinarizer()\n",
    "y = target_preprocessor.fit_transform(y.values).ravel()\n",
    "for i, c in enumerate(target_preprocessor.classes_):\n",
    "    print(f'Label `{c}` encoded into: `{i}`')\n",
    "print(f'Transformed target shape: {y.shape}')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, stratify = y, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "### mandatotry fields - age, sex\n",
    "### custom logic field - education_num\n",
    "categorical = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "categorical_preproc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "zero_imputed = ['fnlgt', 'capital_gain', 'capital_loss']\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "median_imputed = ['hours_per_week']\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_preproc, categorical),\n",
    "        ('zero_imputed', zero_imputer, zero_imputed),\n",
    "        ('median_imputed', median_imputer, median_imputed)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train[num_features + cat_features])\n",
    "print(f'Transformed train data shape: {X_train.shape}')\n",
    "\n",
    "X_test = preprocessor.transform(X_test[num_features + cat_features])\n",
    "print(f'Transformed test data shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting classifier (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, ConfusionMatrixDisplay, PrecisionRecallDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state = RANDOM_STATE)\n",
    "gbc.fit(X_train, y_train)\n",
    "# make the prediction using the resulting model\n",
    "y_pred = gbc.predict(X_test)\n",
    "y_pred_proba = gbc.predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    display_labels = target_preprocessor.classes_,\n",
    "    ax = axes[0]\n",
    ")\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_proba[:, 1],\n",
    "    ax = axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing exporting and loading model and running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "MODEL_PATH = 'model_pipeline.joblib'\n",
    "\n",
    "model_pipeline = {\n",
    "    'ind_features': num_features + cat_features,\n",
    "    'target': target,\n",
    "    'preprocessor': preprocessor,\n",
    "    'target_preprocessor': target_preprocessor,\n",
    "    'classifier': gbc\n",
    "}\n",
    "\n",
    "dump(model_pipeline, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(MODEL_PATH)\n",
    "\n",
    "y = df[model['target']]\n",
    "X = df.loc[:, df.columns != model['target']]\n",
    "\n",
    "y = model['target_preprocessor'].transform(y).ravel()\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size = TEST_SIZE, stratify = y, random_state = RANDOM_STATE)\n",
    "\n",
    "X_test = model['preprocessor'].transform(X_test)\n",
    "\n",
    "y_pred = model['classifier'].predict(X_test)\n",
    "y_pred_proba = model['classifier'].predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost classifier (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(random_state = RANDOM_STATE)\n",
    "abc.fit(X_train, y_train)\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "y_pred = abc.predict(X_test)\n",
    "y_pred_proba = abc.predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    display_labels = target_preprocessor.classes_,\n",
    "    ax = axes[0]\n",
    ")\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_proba[:, 1],\n",
    "    ax = axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_pred_proba = rfc.predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    display_labels = target_preprocessor.classes_,\n",
    "    ax = axes[0]\n",
    ")\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_proba[:, 1],\n",
    "    ax = axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state = RANDOM_STATE)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_proba = lr.predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    display_labels = target_preprocessor.classes_,\n",
    "    ax = axes[0]\n",
    ")\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_proba[:, 1],\n",
    "    ax = axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost - does not require preprocessing of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL for catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target]\n",
    "X = df.loc[:, df.columns != target]\n",
    "\n",
    "y = target_preprocessor.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, stratify = y, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "cbc = CatBoostClassifier(\n",
    "    cat_features = cat_features,\n",
    "    random_seed = RANDOM_STATE,\n",
    "    verbose = False)\n",
    "\n",
    "# train the model\n",
    "cbc.fit(X_train, y_train)\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "y_pred = cbc.predict(X_test)\n",
    "y_pred_proba = cbc.predict_proba(X_test)\n",
    "\n",
    "fbeta = fbeta_score(y_test, y_pred, beta=1, zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'F1 beta score        : {fbeta:.3f}')\n",
    "print(f'Precision (PPV) score: {precision:.3f}')\n",
    "print(f'Recal (TPR) score    : {recall:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    display_labels = target_preprocessor.classes_,\n",
    "    ax = axes[0]\n",
    ")\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_proba[:, 1],\n",
    "    ax = axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target feature plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables - Plots without percentile filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots((len(cat_features) + 2) // 3, 3, figsize = (20, 20))\n",
    "\n",
    "for i, cat_f in enumerate(cat_features):\n",
    "    ix = i % 3\n",
    "    iy = i // 3\n",
    "    sns.countplot(\n",
    "        data = df,\n",
    "        y = cat_f,\n",
    "        hue = target,\n",
    "        ax = axes[iy, ix],\n",
    "        order = cat_order[cat_f]\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables - Plots with percentile filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_get_percentile(df, cat_f, target, percentile = 0.95):\n",
    "    \n",
    "    tmp_df = df[[cat_f, target]].copy(deep = True)\n",
    "    value_counts = tmp_df[cat_f].value_counts()\n",
    "    within_percentile = value_counts[value_counts.cumsum() <= percentile * tmp_df.shape[0]].index.tolist()\n",
    "    tmp_df[cat_f] = tmp_df[cat_f].map(lambda x: x if x in within_percentile else 'Other')\n",
    "\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots((len(cat_features) + 2) // 3, 3, figsize = (20, 15))\n",
    "\n",
    "for i, cat_f in enumerate(cat_features):\n",
    "    tmp_df = _transform_get_percentile(df, cat_f, target)\n",
    "    ix = i % 3\n",
    "    iy = i // 3\n",
    "    tmp_order = [v for v in cat_order[cat_f] if v in tmp_df[cat_f].unique()] \\\n",
    "                if cat_order[cat_f] is not None else None\n",
    "    sns.countplot(\n",
    "        data = tmp_df,\n",
    "        y = cat_f,\n",
    "        hue = target,\n",
    "        ax = axes[iy, ix],\n",
    "        order = tmp_order\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots((len(num_features) + 2) // 3, 3, figsize = (20, 10))\n",
    "\n",
    "for i, num_f in enumerate(num_features):\n",
    "    ix = i % 3\n",
    "    iy = i // 3\n",
    "    sns.kdeplot(\n",
    "        data = df, \n",
    "        x = num_f,\n",
    "        hue = target,\n",
    "        ax = axes[iy, ix],\n",
    "        fill=True\n",
    "    )\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent variables plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x = 'age',\n",
    "    y = 'occupation',\n",
    "    hue = target,\n",
    "    kind = 'box',\n",
    "    data = df\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = df['age']\n",
    "assert age.between(17, 90).all()\n",
    "age.hist(bins = 74)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='age',\n",
    "    y='salary',\n",
    "    kind='violin',\n",
    "    data=df\n",
    " )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='age',\n",
    "    y='salary',\n",
    "    kind='box',\n",
    "    data=df\n",
    " )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `workclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass = df['workclass']\n",
    "workclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fnlgt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnlgt = df['fnlgt']\n",
    "pd.DataFrame(data = {'10 largest': fnlgt.nlargest(10).values, '10 smallest': fnlgt.nsmallest(10).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnlgt.hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = df['education']\n",
    "education.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `education_num`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_num = df['education_num']\n",
    "education_num.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `marital_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_status = df['marital_status']\n",
    "marital_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `occupation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation = df['occupation']\n",
    "occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relationship`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship = df['relationship']\n",
    "relationship.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `race`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = df['race']\n",
    "race.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = df['sex']\n",
    "sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `capital_gain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_gain = df['capital_gain']\n",
    "pd.DataFrame(data = {'10 largest': capital_gain.nlargest(10).values, '10 smallest': capital_gain.nsmallest(10).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_gain[(capital_gain != 0) & (capital_gain != 99999)].hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `capital_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_loss = df['capital_loss']\n",
    "pd.DataFrame(data = {'10 largest': capital_loss.nlargest(10).values, '10 smallest': capital_loss.nsmallest(10).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_loss[capital_loss != 0].hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `hours_per_week`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_per_week = df['hours_per_week']\n",
    "pd.DataFrame(data = {'10 largest': hours_per_week.nlargest(10).values, '10 smallest': hours_per_week.nsmallest(10).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_per_week.hist(bins = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `native_country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_country = df['native_country']\n",
    "native_country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target]\n",
    "X = df.loc[:, df.columns != target]\n",
    "\n",
    "y = target_preprocessor.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, stratify = y, random_state = RANDOM_STATE)\n",
    "\n",
    "df_aq = X_test.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq['label_value'] = y_test\n",
    "df_aq['score'] = y_pred\n",
    "df_aq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcut_cols = [n for n in num_features if n not in ['fnlgt', 'education_num', 'capital_gain', 'capital_loss']]\n",
    "for num_f in qcut_cols:\n",
    "    df_aq[num_f + '_qcut'] = pd.qcut(df_aq[num_f], 10, duplicates = 'drop').astype(str)\n",
    "aq_cols = cat_features + [n + '_qcut' for n in qcut_cols] + ['label_value', 'score']\n",
    "df_aq = df_aq.loc[:, aq_cols]\n",
    "df_aq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge small group frequencies\n",
    "def _transform_get_percentile(df, cat_f, target, percentile = 0.95):\n",
    "    \n",
    "    tmp_df = df[[cat_f, target]].copy(deep = True)\n",
    "    value_counts = tmp_df[cat_f].value_counts()\n",
    "    within_percentile = value_counts[value_counts.cumsum() <= percentile * tmp_df.shape[0]].index.tolist()\n",
    "    tmp_df[cat_f] = tmp_df[cat_f].map(lambda x: x if x in within_percentile else 'Other')\n",
    "\n",
    "    return tmp_df\n",
    "\n",
    "for cat_f in cat_features:\n",
    "    df_aq[cat_f] = _transform_get_percentile(df, cat_f, target)[cat_f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases across subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias \n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot\n",
    "\n",
    "aqp = Plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = Group()\n",
    "xtab, idxs = group.get_crosstabs(df_aq)\n",
    "absolute_metrics = group.list_absolute_metrics(xtab)\n",
    "\n",
    "# xtab[[col for col in xtab.columns if col not in absolute_metrics + ['model_id', 'score_threshold', 'k', 'pp', 'pn', 'group_label_pos', 'group_label_neg']]]\n",
    "group_size_fr = xtab['group_size'] / xtab['total_entities']\n",
    "xtab.loc[group_size_fr >= 0.05, ['attribute_name', 'attribute_value', 'tp', 'fp', 'tn', 'fn', 'tpr', 'tnr', 'precision', 'npv', 'group_size', 'total_entities']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnr = aqp.plot_group_metric_all(xtab[(group_size_fr >= 0.01) & xtab['attribute_name'].isin(['marital_status', 'race', 'sex'])], ncols = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparsity levels across groups w.r.t. majority group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define major groups\n",
    "ref_group = {}\n",
    "for c in cat_features + [n + '_qcut' for n in qcut_cols]:\n",
    "    ref_group[c] = df_aq[c].value_counts().index[0]\n",
    "\n",
    "bias = Bias()\n",
    "bias_df = bias.get_disparity_predefined_groups(\n",
    "    xtab, original_df = df_aq,\n",
    "    ref_groups_dict = ref_group,\n",
    "    alpha = 0.05, check_significance = True, mask_significance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'npv'\n",
    "bias_df.loc[\n",
    "    (bias_df[metric + '_significance'] == True) & bias_df['attribute_name'].isin(['marital_status', 'race', 'sex', 'native_country']), \n",
    "    ['attribute_name', 'attribute_value', 'group_size', 'label_value_significance', 'score_significance', metric + '_disparity', metric + '_significance', metric + '_ref_group_value']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aqp.plot_disparity_all(bias_df, metrics = [metric + '_disparity'], significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aqp.plot_disparity_all(bias_df, attributes = ['native_country'], significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and group level fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness = Fairness()\n",
    "fairness_df = fairness.get_group_value_fairness(bias_df)\n",
    "\n",
    "gof = fairness.get_overall_fairness(fairness_df)\n",
    "gof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aqp.plot_fairness_disparity_all(bias_df, metrics=[metric + '_disparity'], significance_alpha=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('udacity-mldevops-3rd-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7398f253271b3d820258efcb907d010d4e8443461b83c64c13eac9ca16b05d4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
